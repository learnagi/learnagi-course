---
title: "æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
slug: "basics"
sequence: 1
description: "æ·±å…¥è§£ææ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯ï¼Œä»åŸç†åˆ°å®è·µï¼Œå¸®åŠ©ä½ æ„å»ºåŸºäºå‘é‡æ•°æ®åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"
is_published: true
estimated_minutes: 45
language: "zh-CN"
permalink: "https://www.agi01.com/course/agi/rag/basics"
course: "agi/course/rag"
header_image: "images/basics_header.png"
---

# æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

![æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ](images/basics_header.png)

## RAG æ˜¯ä»€ä¹ˆï¼ŸğŸ¤”

æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼ŒRAGï¼‰æ˜¯ä¸€ç§å°†æ£€ç´¢ç³»ç»Ÿä¸ç”Ÿæˆå¼AIæ¨¡å‹ç»“åˆçš„æŠ€æœ¯ã€‚å®ƒé€šè¿‡æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºAIæ¨¡å‹çš„å›ç­”èƒ½åŠ›ï¼Œä½¿å›ç­”æ›´å‡†ç¡®ã€æ›´å¯é ã€‚

### å·¥ä½œåŸç†
```python
from langchain import OpenAI, VectorDBQA
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import TextLoader

class RAGSystem:
    def __init__(self, api_key):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        self.llm = OpenAI(openai_api_key=api_key)
        self.vector_store = None
        
    def load_documents(self, file_path):
        """åŠ è½½æ–‡æ¡£"""
        loader = TextLoader(file_path)
        documents = loader.load()
        
        # æ–‡æœ¬åˆ†å‰²
        text_splitter = CharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = Chroma.from_documents(
            texts,
            self.embeddings
        )
        
    def query(self, question):
        """æŸ¥è¯¢é—®é¢˜"""
        if not self.vector_store:
            raise ValueError("è¯·å…ˆåŠ è½½æ–‡æ¡£")
            
        qa = VectorDBQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            vectorstore=self.vector_store
        )
        
        return qa.run(question)
```

## æ ¸å¿ƒç»„ä»¶ ğŸ”§

### 1. æ–‡æ¡£å¤„ç†
```python
from typing import List, Dict
import numpy as np

class DocumentProcessor:
    def __init__(self, chunk_size=1000, overlap=200):
        self.chunk_size = chunk_size
        self.overlap = overlap
        
    def process_document(self, content: str) -> List[Dict]:
        """å¤„ç†æ–‡æ¡£å†…å®¹"""
        # åˆ†å‰²æ–‡æœ¬
        chunks = self._split_text(content)
        
        # æ¸…ç†å’Œè§„èŒƒåŒ–
        cleaned_chunks = [self._clean_text(chunk) for chunk in chunks]
        
        # æå–å…ƒæ•°æ®
        documents = [
            {
                'content': chunk,
                'metadata': self._extract_metadata(chunk)
            }
            for chunk in cleaned_chunks
        ]
        
        return documents
    
    def _split_text(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬"""
        # å®ç°æ™ºèƒ½åˆ†å‰²é€»è¾‘
        chunks = []
        start = 0
        
        while start < len(text):
            # æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹
            end = start + self.chunk_size
            if end < len(text):
                # å¯»æ‰¾å¥å­è¾¹ç•Œ
                while end > start and text[end] not in '.!?':
                    end -= 1
                if end == start:
                    end = start + self.chunk_size
            
            chunks.append(text[start:end])
            start = end - self.overlap
            
        return chunks
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†å’Œè§„èŒƒåŒ–æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = ' '.join(text.split())
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',').replace('ã€‚', '.')
        return text
    
    def _extract_metadata(self, chunk: str) -> Dict:
        """æå–æ–‡æœ¬å…ƒæ•°æ®"""
        return {
            'length': len(chunk),
            'sentences': len([s for s in chunk.split('.') if s]),
            'created_at': datetime.now().isoformat()
        }
```

### 2. å‘é‡ç´¢å¼•
```python
class VectorIndex:
    def __init__(self, embedding_dim=768):
        self.embedding_dim = embedding_dim
        self.index = None
        self.documents = []
        
    def build_index(self, documents: List[Dict], embeddings: List[np.ndarray]):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        # ä½¿ç”¨HNSWç´¢å¼•
        self.index = hnswlib.Index(space='cosine', dim=self.embedding_dim)
        self.index.init_index(
            max_elements=len(embeddings),
            ef_construction=200,
            M=16
        )
        
        # æ·»åŠ å‘é‡
        self.index.add_items(
            embeddings,
            np.arange(len(embeddings))
        )
        
        # å­˜å‚¨æ–‡æ¡£
        self.documents = documents
        
    def search(self, query_vector: np.ndarray, k: int = 5) -> List[Dict]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»º")
            
        # æ‰§è¡Œå‘é‡æœç´¢
        labels, distances = self.index.knn_query(query_vector, k)
        
        # è¿”å›ç›¸å…³æ–‡æ¡£
        results = []
        for idx, dist in zip(labels[0], distances[0]):
            results.append({
                'document': self.documents[idx],
                'score': 1 - dist  # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
            })
            
        return results
```

### 3. ä¸Šä¸‹æ–‡æ„å»º
```python
class ContextBuilder:
    def __init__(self, max_tokens=2000):
        self.max_tokens = max_tokens
        
    def build_context(self, retrieved_docs: List[Dict]) -> str:
        """æ„å»ºé—®ç­”ä¸Šä¸‹æ–‡"""
        context = []
        current_tokens = 0
        
        for doc in retrieved_docs:
            # ä¼°ç®—tokenæ•°é‡
            estimated_tokens = len(doc['document']['content'].split()) * 1.3
            
            if current_tokens + estimated_tokens > self.max_tokens:
                break
                
            context.append(doc['document']['content'])
            current_tokens += estimated_tokens
        
        # ç»„ç»‡ä¸Šä¸‹æ–‡
        return self._format_context(context)
    
    def _format_context(self, context_parts: List[str]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        return "\n\nç›¸å…³ä¿¡æ¯ï¼š\n" + "\n---\n".join(context_parts)
```

### 4. å“åº”ç”Ÿæˆ
```python
from typing import Optional

class ResponseGenerator:
    def __init__(self, model_name="gpt-3.5-turbo"):
        self.model_name = model_name
        self.client = OpenAI()
        
    def generate_response(
        self,
        question: str,
        context: str,
        max_tokens: int = 500
    ) -> Dict:
        """ç”Ÿæˆå›ç­”"""
        # æ„å»ºæç¤ºè¯
        prompt = self._build_prompt(question, context)
        
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        return {
            'answer': response.choices[0].message.content,
            'model': self.model_name,
            'tokens_used': response.usage.total_tokens
        }
    
    def _build_prompt(self, question: str, context: str) -> str:
        """æ„å»ºæç¤ºè¯"""
        return f"""è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€å®Œæ•´çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚"""
```

## å®è·µæ¡ˆä¾‹ ğŸ’¡

### 1. æ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
```python
class DocumentQA:
    def __init__(self, api_key: str):
        self.processor = DocumentProcessor()
        self.embedder = OpenAIEmbeddings(api_key=api_key)
        self.vector_index = VectorIndex()
        self.context_builder = ContextBuilder()
        self.generator = ResponseGenerator()
        
    def load_documents(self, file_paths: List[str]):
        """åŠ è½½æ–‡æ¡£"""
        all_documents = []
        all_embeddings = []
        
        for path in file_paths:
            # è¯»å–æ–‡ä»¶
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å¤„ç†æ–‡æ¡£
            documents = self.processor.process_document(content)
            
            # ç”ŸæˆåµŒå…¥
            embeddings = [
                self.embedder.embed_query(doc['content'])
                for doc in documents
            ]
            
            all_documents.extend(documents)
            all_embeddings.extend(embeddings)
        
        # æ„å»ºç´¢å¼•
        self.vector_index.build_index(all_documents, all_embeddings)
        
    def answer_question(self, question: str) -> Dict:
        """å›ç­”é—®é¢˜"""
        # ç”Ÿæˆé—®é¢˜çš„åµŒå…¥å‘é‡
        query_vector = self.embedder.embed_query(question)
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        relevant_docs = self.vector_index.search(query_vector)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self.context_builder.build_context(relevant_docs)
        
        # ç”Ÿæˆå›ç­”
        response = self.generator.generate_response(question, context)
        
        return {
            'answer': response['answer'],
            'sources': [doc['document']['content'][:200] + "..."
                       for doc in relevant_docs],
            'model_info': {
                'model': response['model'],
                'tokens': response['tokens_used']
            }
        }
```

### 2. ä½¿ç”¨ç¤ºä¾‹
```python
# åˆå§‹åŒ–ç³»ç»Ÿ
qa_system = DocumentQA(api_key="your-api-key")

# åŠ è½½æ–‡æ¡£
qa_system.load_documents([
    "company_policies.txt",
    "product_manual.pdf",
    "faq.md"
])

# æé—®
question = "æˆ‘ä»¬çš„é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"
response = qa_system.answer_question(question)

print("å›ç­”:", response['answer'])
print("\næ¥æºæ–‡æ¡£:")
for source in response['sources']:
    print("-", source)
print("\nä½¿ç”¨æ¨¡å‹:", response['model_info']['model'])
print("æ¶ˆè€—Token:", response['model_info']['tokens'])
```

## ä¼˜åŒ–æŠ€å·§ ğŸš€

### 1. æé«˜æ£€ç´¢è´¨é‡
- åˆç†è®¾ç½®æ–‡æ¡£åˆ†å—å¤§å°
- ä½¿ç”¨é‡å æ¥ä¿æŒä¸Šä¸‹æ–‡
- ä¼˜åŒ–å‘é‡ç´¢å¼•å‚æ•°

### 2. å¢å¼ºå›ç­”å‡†ç¡®æ€§
- ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹
- æ·»åŠ ç›¸å…³æ€§è¿‡æ»¤
- ä½¿ç”¨å¤šè½®å¯¹è¯

### 3. æ€§èƒ½ä¼˜åŒ–
- å®ç°ç»“æœç¼“å­˜
- æ‰¹é‡å¤„ç†æ–‡æ¡£
- å¼‚æ­¥å¤„ç†è¯·æ±‚

## æ³¨æ„äº‹é¡¹ âš ï¸

1. **æ•°æ®å®‰å…¨**
   - ä¿æŠ¤æ•æ„Ÿä¿¡æ¯
   - å®ç°è®¿é—®æ§åˆ¶
   - å®šæœŸæ›´æ–°æ–‡æ¡£

2. **æˆæœ¬æ§åˆ¶**
   - ä¼˜åŒ–Tokenä½¿ç”¨
   - å®ç°ç¼“å­˜æœºåˆ¶
   - ç›‘æ§APIè°ƒç”¨

3. **ç³»ç»Ÿç»´æŠ¤**
   - å®šæœŸæ›´æ–°ç´¢å¼•
   - ç›‘æ§ç³»ç»Ÿæ€§èƒ½
   - å¤„ç†å¼‚å¸¸æƒ…å†µ

## å°ç»“ ğŸ“

æœ¬ç« æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†RAGæŠ€æœ¯ï¼š

1. **åŸºç¡€æ¦‚å¿µ**
   - RAGåŸç†
   - æ ¸å¿ƒç»„ä»¶
   - å·¥ä½œæµç¨‹

2. **å®ç°ç»†èŠ‚**
   - æ–‡æ¡£å¤„ç†
   - å‘é‡ç´¢å¼•
   - ä¸Šä¸‹æ–‡æ„å»º
   - å“åº”ç”Ÿæˆ

3. **å®è·µåº”ç”¨**
   - ç³»ç»Ÿæ„å»º
   - ä¼˜åŒ–æŠ€å·§
   - æ³¨æ„äº‹é¡¹

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†æ¢è®¨å‘é‡æ•°æ®åº“çš„æ›´å¤šå®é™…åº”ç”¨åœºæ™¯ï¼
