---
title: "è¯­è¨€æ¨¡å‹ä½¿ç”¨ï¼šæŒæ¡LangChainä¸­çš„æ¨¡å‹æ“ä½œ"
slug: "models"
sequence: 2
description: "æ·±å…¥äº†è§£LangChainä¸­çš„å„ç±»æ¨¡å‹åŠå…¶ä½¿ç”¨æ–¹æ³•"
is_published: true
estimated_minutes: 30
language: "zh-CN"
permalink: "https://www.agi01.com/course/agi/langchain/models"
course: "agi/course/langchain"
header_image: "images/models_header.png"
---

# è¯­è¨€æ¨¡å‹ä½¿ç”¨ï¼šæŒæ¡LangChainä¸­çš„æ¨¡å‹æ“ä½œ

## æ¨¡å‹ç±»å‹æ¦‚è§ˆ ğŸ—‚ï¸

LangChainæ”¯æŒå¤šç§ç±»å‹çš„æ¨¡å‹ï¼š

1. **LLMsï¼ˆä¼ ç»Ÿè¯­è¨€æ¨¡å‹ï¼‰**
   - è¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºæ–‡æœ¬
   - é€‚åˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡

2. **Chat Modelsï¼ˆèŠå¤©æ¨¡å‹ï¼‰**
   - æ”¯æŒå¤šè½®å¯¹è¯
   - ç†è§£å¯¹è¯ä¸Šä¸‹æ–‡

3. **Embeddingsï¼ˆåµŒå…¥æ¨¡å‹ï¼‰**
   - å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
   - ç”¨äºç›¸ä¼¼åº¦è®¡ç®—

## LLMsçš„ä½¿ç”¨ ğŸ“

### 1. åŸºæœ¬ä½¿ç”¨

```python
from langchain.llms import OpenAI

# åˆ›å»ºLLM
llm = OpenAI(temperature=0.7)

# ç”Ÿæˆæ–‡æœ¬
text = llm.invoke("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")
print(text)
# è¾“å‡ºï¼š
# æ˜¥é£è½»æŠšç»¿æŸ³æ¢¢ï¼Œ
# ç™¾èŠ±äº‰è‰³é—¹æå¤´ã€‚
# è´è¶ç¿©ç¿©èˆæ˜¥å…‰ï¼Œ
# ç»†é›¨æ¶¦ç‰©çš†æ¸©æŸ”ã€‚
```

### 2. æ‰¹é‡å¤„ç†

```python
# å‡†å¤‡å¤šä¸ªæç¤º
prompts = [
    "å†™ä¸€å¥å…³äºæ˜¥å¤©çš„è¯—",
    "å†™ä¸€å¥å…³äºå¤å¤©çš„è¯—",
    "å†™ä¸€å¥å…³äºç§‹å¤©çš„è¯—"
]

# æ‰¹é‡ç”Ÿæˆ
results = llm.batch(prompts)
for prompt, result in zip(prompts, results):
    print(f"æç¤º: {prompt}")
    print(f"ç”Ÿæˆ: {result}\n")
```

### 3. æµå¼è¾“å‡º

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# åˆ›å»ºæµå¼LLM
streaming_llm = OpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0.7
)

# ç”Ÿæˆæ–‡æœ¬ï¼ˆå®æ—¶è¾“å‡ºï¼‰
streaming_llm.invoke("è®²ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ•…äº‹")
```

## Chat Modelsçš„ä½¿ç”¨ ğŸ’­

### 1. åŸºæœ¬å¯¹è¯

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage, SystemMessage

# åˆ›å»ºèŠå¤©æ¨¡å‹
chat = ChatOpenAI(temperature=0)

# è¿›è¡Œå¯¹è¯
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å‹å¥½çš„åŠ©æ‰‹"),
    HumanMessage(content="ä½ å¥½ï¼"),
    AIMessage(content="ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚"),
    HumanMessage(content="ä»Šå¤©å¤©æ°”çœŸå¥½ï¼")
]

response = chat.invoke(messages)
print(response.content)
# è¾“å‡º: æ˜¯çš„ï¼Œç¡®å®æ˜¯ä¸ªå¥½å¤©æ°”ï¼å¸Œæœ›ä½ èƒ½å¥½å¥½äº«å—è¿™ç¾å¥½çš„ä¸€å¤©ã€‚
```

### 2. è§’è‰²æ‰®æ¼”

```python
# åˆ›å»ºç‰¹å®šè§’è‰²çš„åŠ©æ‰‹
messages = [
    SystemMessage(content="""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„Pythonç¼–ç¨‹æ•™å¸ˆã€‚
    - ä½¿ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šæ¦‚å¿µ
    - æä¾›å…·ä½“çš„ä»£ç ç¤ºä¾‹
    - é¼“åŠ±å­¦ç”Ÿæ€è€ƒå’Œå®è·µ
    """),
    HumanMessage(content="ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ")
]

response = chat.invoke(messages)
print(response.content)
# è¾“å‡ºï¼š
# è®©æˆ‘ç”¨ç®€å•çš„æ–¹å¼è§£é‡Šåˆ—è¡¨æ¨å¯¼å¼ï¼š
# 
# åˆ—è¡¨æ¨å¯¼å¼æ˜¯Pythonä¸­ä¸€ç§ç®€æ´åœ°åˆ›å»ºåˆ—è¡¨çš„æ–¹æ³•ã€‚æƒ³è±¡ä½ æœ‰ä¸€ä¸ª"åŠ å·¥æµæ°´çº¿"ï¼š
# 
# 1. åŸºæœ¬è¯­æ³•ï¼š
# [è¡¨è¾¾å¼ for å…ƒç´  in å¯è¿­ä»£å¯¹è±¡]
# 
# ä¸¾ä¸ªä¾‹å­ï¼š
# ```python
# # åˆ›å»º1-5çš„å¹³æ–¹æ•°åˆ—è¡¨
# squares = [x**2 for x in range(1, 6)]
# print(squares)  # [1, 4, 9, 16, 25]
# ```
# 
# è¯•è¯•è‡ªå·±å†™ä¸€ä¸ªï¼ŸæŠŠä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨è½¬æ¢æˆå¤§å†™ï¼š
# ```python
# words = ["hello", "world", "python"]
# upper_words = [word.upper() for word in words]
# ```
```

### 3. å¤šè½®å¯¹è¯ç®¡ç†

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆ›å»ºå¸¦è®°å¿†çš„å¯¹è¯é“¾
conversation = ConversationChain(
    llm=ChatOpenAI(),
    memory=ConversationBufferMemory()
)

# è¿›è¡Œå¤šè½®å¯¹è¯
print(conversation.predict(input="ä½ å¥½ï¼"))
print(conversation.predict(input="æˆ‘å«å°æ˜"))
print(conversation.predict(input="è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ"))
# è¾“å‡ºï¼š
# ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚
# ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚
# å½“ç„¶è®°å¾—ï¼Œä½ æ˜¯å°æ˜ï¼
```

## Embeddingsçš„ä½¿ç”¨ ğŸ”¤

### 1. æ–‡æœ¬å‘é‡åŒ–

```python
from langchain.embeddings import OpenAIEmbeddings

# åˆ›å»ºåµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings()

# è·å–æ–‡æœ¬å‘é‡
text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
vector = embeddings.embed_query(text)
print(f"å‘é‡ç»´åº¦: {len(vector)}")  # 1536
```

### 2. æ‰¹é‡å¤„ç†

```python
# å‡†å¤‡å¤šä¸ªæ–‡æœ¬
texts = [
    "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é¢†åŸŸ",
    "æ·±åº¦å­¦ä¹ æ¨åŠ¨äº†AIçš„å‘å±•"
]

# æ‰¹é‡è·å–å‘é‡
vectors = embeddings.embed_documents(texts)
print(f"æ–‡æœ¬æ•°é‡: {len(vectors)}")
print(f"æ¯ä¸ªå‘é‡çš„ç»´åº¦: {len(vectors[0])}")
```

### 3. ç›¸ä¼¼åº¦è®¡ç®—

```python
import numpy as np

def cosine_similarity(v1, v2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

# å‡†å¤‡æ–‡æœ¬
text1 = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ"
text2 = "AIæŠ€æœ¯æ¨åŠ¨ç¤¾ä¼šå‘å±•"
text3 = "ä»Šå¤©å¤©æ°”çœŸå¥½"

# è·å–å‘é‡
vec1 = embeddings.embed_query(text1)
vec2 = embeddings.embed_query(text2)
vec3 = embeddings.embed_query(text3)

# è®¡ç®—ç›¸ä¼¼åº¦
sim12 = cosine_similarity(vec1, vec2)
sim13 = cosine_similarity(vec1, vec3)

print(f"{text1} å’Œ {text2} çš„ç›¸ä¼¼åº¦: {sim12:.4f}")
print(f"{text1} å’Œ {text3} çš„ç›¸ä¼¼åº¦: {sim13:.4f}")
# è¾“å‡ºï¼š
# äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ å’Œ AIæŠ€æœ¯æ¨åŠ¨ç¤¾ä¼šå‘å±• çš„ç›¸ä¼¼åº¦: 0.8912
# äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ å’Œ ä»Šå¤©å¤©æ°”çœŸå¥½ çš„ç›¸ä¼¼åº¦: 0.2134
```

## æ¨¡å‹å‚æ•°è°ƒä¼˜ âš™ï¸

### 1. æ¸©åº¦æ§åˆ¶

```python
# åˆ›å»ºä¸åŒæ¸©åº¦çš„æ¨¡å‹
creative_llm = OpenAI(temperature=0.9)  # æ›´æœ‰åˆ›æ„
balanced_llm = OpenAI(temperature=0.5)  # å¹³è¡¡
precise_llm = OpenAI(temperature=0)     # æ›´ç¡®å®š

prompt = "ç»™ä¸€ä¸ªç§‘å¹»æ•…äº‹çš„å¼€å¤´"

print("åˆ›æ„ç‰ˆæœ¬ï¼š")
print(creative_llm.invoke(prompt))

print("\nå¹³è¡¡ç‰ˆæœ¬ï¼š")
print(balanced_llm.invoke(prompt))

print("\nç²¾ç¡®ç‰ˆæœ¬ï¼š")
print(precise_llm.invoke(prompt))
```

### 2. è¾“å‡ºæ§åˆ¶

```python
# æ§åˆ¶è¾“å‡ºé•¿åº¦
short_llm = OpenAI(max_tokens=50)
long_llm = OpenAI(max_tokens=200)

# æ§åˆ¶åœæ­¢æ ‡è®°
custom_llm = OpenAI(
    stop=["\n", "ã€‚"],  # é‡åˆ°æ¢è¡Œæˆ–å¥å·åœæ­¢
    max_tokens=100
)

# ç¤ºä¾‹
prompt = "å†™ä¸€ä¸ªå…³äºæœªæ¥ç§‘æŠ€çš„æ•…äº‹"
print("ç®€çŸ­ç‰ˆæœ¬ï¼š")
print(short_llm.invoke(prompt))

print("\nè¯¦ç»†ç‰ˆæœ¬ï¼š")
print(long_llm.invoke(prompt))
```

### 3. é‡‡æ ·ç­–ç•¥

```python
# ä½¿ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥
from langchain.llms import OpenAI

# Top-Pé‡‡æ ·
creative_llm = OpenAI(
    temperature=0.7,
    top_p=0.9
)

# Top-Ké‡‡æ ·
focused_llm = OpenAI(
    temperature=0.7,
    top_k=40
)

# é¢‘ç‡æƒ©ç½š
diverse_llm = OpenAI(
    temperature=0.7,
    frequency_penalty=0.5,
    presence_penalty=0.5
)
```

## æœ€ä½³å®è·µ âœ¨

### 1. æ¨¡å‹é€‰æ‹©

```python
def choose_model(task_type, requirements):
    """æ ¹æ®ä»»åŠ¡ç±»å‹å’Œè¦æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""
    if task_type == "å¯¹è¯":
        if requirements.get("åˆ›é€ æ€§", False):
            return ChatOpenAI(temperature=0.8)
        else:
            return ChatOpenAI(temperature=0)
    
    elif task_type == "ç”Ÿæˆ":
        if requirements.get("é•¿æ–‡æœ¬", False):
            return OpenAI(
                temperature=0.7,
                max_tokens=500
            )
        else:
            return OpenAI(
                temperature=0.5,
                max_tokens=100
            )
    
    elif task_type == "åˆ†æ":
        return OpenAI(temperature=0)
```

### 2. é”™è¯¯å¤„ç†

```python
import time
from typing import Optional

class RobustModel:
    def __init__(self, model, max_retries=3, delay=1):
        self.model = model
        self.max_retries = max_retries
        self.delay = delay
    
    def invoke(self, prompt: str) -> Optional[str]:
        """å¸¦é‡è¯•çš„æ¨¡å‹è°ƒç”¨"""
        for attempt in range(self.max_retries):
            try:
                return self.model.invoke(prompt)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    print(f"æœ€ç»ˆé”™è¯¯: {str(e)}")
                    return None
                print(f"å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                time.sleep(self.delay * (attempt + 1))
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
class ModelManager:
    def __init__(self):
        self._models = {}
        
    def get_model(self, model_type: str, **kwargs):
        """è·å–æˆ–åˆ›å»ºæ¨¡å‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
        key = f"{model_type}_{hash(frozenset(kwargs.items()))}"
        
        if key not in self._models:
            if model_type == "chat":
                self._models[key] = ChatOpenAI(**kwargs)
            elif model_type == "llm":
                self._models[key] = OpenAI(**kwargs)
            elif model_type == "embeddings":
                self._models[key] = OpenAIEmbeddings(**kwargs)
                
        return self._models[key]

# ä½¿ç”¨ç¤ºä¾‹
manager = ModelManager()

# è·å–ç›¸åŒé…ç½®çš„æ¨¡å‹å®ä¾‹ï¼ˆå¤ç”¨ï¼‰
chat1 = manager.get_model("chat", temperature=0)
chat2 = manager.get_model("chat", temperature=0)
print(chat1 is chat2)  # True

# è·å–ä¸åŒé…ç½®çš„æ¨¡å‹å®ä¾‹ï¼ˆæ–°å»ºï¼‰
chat3 = manager.get_model("chat", temperature=0.5)
print(chat1 is chat3)  # False
```

## å°ç»“ ğŸ“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š
1. ä¸åŒç±»å‹çš„æ¨¡å‹åŠå…¶ç‰¹ç‚¹
2. å¦‚ä½•ä½¿ç”¨å’Œé…ç½®å„ç§æ¨¡å‹
3. æ¨¡å‹å‚æ•°è°ƒä¼˜çš„æ–¹æ³•
4. å®ç”¨çš„æœ€ä½³å®è·µ

å…³é”®ç‚¹ï¼š
- é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç±»å‹
- æ­£ç¡®è®¾ç½®æ¨¡å‹å‚æ•°
- åšå¥½é”™è¯¯å¤„ç†
- æ³¨æ„æ€§èƒ½ä¼˜åŒ–

ä¸‹ä¸€æ­¥ï¼š
- æ¢ç´¢æ›´å¤šæ¨¡å‹ç±»å‹
- å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
- åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨
