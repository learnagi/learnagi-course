---
title: "LangChainåŸºç¡€ï¼šæ„å»ºæ™ºèƒ½åº”ç”¨"
slug: "basics"
sequence: 1
description: "æŒæ¡LangChainæ¡†æ¶çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸºç¡€ç»„ä»¶ï¼Œå­¦ä¹ å¦‚ä½•æ„å»ºæ™ºèƒ½åº”ç”¨"
is_published: true
estimated_minutes: 45
language: "zh-CN"
permalink: "https://www.agi01.com/course/agi/langchain/basics"
course: "agi/course/langchain"
header_image: "images/basics_header.png"
---

# LangChainåŸºç¡€ï¼šæ„å»ºæ™ºèƒ½åº”ç”¨

![LangChainåŸºç¡€ï¼šæ„å»ºæ™ºèƒ½åº”ç”¨](images/basics_header.png)

## ä»€ä¹ˆæ˜¯ LangChainï¼ŸğŸ¤”

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿï¼š
- è½»æ¾æ„å»ºåŸºäº LLM çš„åº”ç”¨
- ç»„åˆä¸åŒçš„ AI èƒ½åŠ›
- åˆ›å»ºæ™ºèƒ½å·¥ä½œæµç¨‹

å°±åƒä¹é«˜ç§¯æœ¨ä¸€æ ·ï¼ŒLangChain æä¾›äº†å„ç§å¯ç»„åˆçš„ç»„ä»¶ï¼Œè®©æˆ‘ä»¬èƒ½å¿«é€Ÿæ­å»ºæ™ºèƒ½åº”ç”¨ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹© LangChainï¼Ÿ

1. **å¼€ç®±å³ç”¨**
   - ä¸°å¯Œçš„ç»„ä»¶åº“
   - å®Œæ•´çš„å·¥å…·é“¾
   - ç®€å•çš„æ¥å£

2. **çµæ´»å¯æ‰©å±•**
   - è‡ªå®šä¹‰ç»„ä»¶
   - æ’ä»¶æœºåˆ¶
   - å¤šç§é›†æˆ

3. **ç”Ÿäº§å¯ç”¨**
   - æ€§èƒ½ä¼˜åŒ–
   - é”™è¯¯å¤„ç†
   - ç›‘æ§æ”¯æŒ

## æ ¸å¿ƒæ¦‚å¿µ ğŸ“š

### 1. Chainsï¼ˆé“¾ï¼‰

é“¾å°±åƒæ˜¯ä¸€æ¡ç”Ÿäº§çº¿ï¼Œå°†ä¸åŒçš„å¤„ç†æ­¥éª¤ä¸²è”èµ·æ¥ï¼š

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["product"],
    template="ç»™æˆ‘ä»‹ç»ä¸‹{product}çš„ä¸»è¦ç‰¹ç‚¹å’Œä¼˜åŠ¿"
)

# åˆ›å»ºLLM
llm = OpenAI(temperature=0.7)

# åˆ›å»ºé“¾
chain = LLMChain(
    llm=llm,
    prompt=prompt
)

# è¿è¡Œé“¾
response = chain.run("iPhone 15")
print(response)
```

### 2. Promptsï¼ˆæç¤ºï¼‰

æç¤ºæ˜¯ä¸ LLM äº¤äº’çš„å…³é”®ï¼š

```python
# ç®€å•çš„æç¤ºæ¨¡æ¿
simple_prompt = PromptTemplate(
    input_variables=["question"],
    template="è¯·å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{question}"
)

# å¸¦æœ‰ç¤ºä¾‹çš„æç¤ºæ¨¡æ¿
few_shot_prompt = FewShotPromptTemplate(
    examples=[
        {"question": "1+1ç­‰äºå‡ ï¼Ÿ", "answer": "1+1ç­‰äº2"},
        {"question": "2+2ç­‰äºå‡ ï¼Ÿ", "answer": "2+2ç­‰äº4"}
    ],
    example_prompt=PromptTemplate(
        input_variables=["question", "answer"],
        template="é—®ï¼š{question}\nç­”ï¼š{answer}"
    ),
    suffix="é—®ï¼š{input}\nç­”ï¼š",
    input_variables=["input"]
)
```

### 3. Memoryï¼ˆè®°å¿†ï¼‰

è®© AI èƒ½å¤Ÿè®°ä½å¯¹è¯å†å²ï¼š

```python
from langchain.memory import ConversationBufferMemory

# åˆ›å»ºè®°å¿†ç»„ä»¶
memory = ConversationBufferMemory()

# åˆ›å»ºå¸¦è®°å¿†çš„é“¾
chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# å¯¹è¯ç¤ºä¾‹
chain.predict(input="ä½ å¥½ï¼")
chain.predict(input="æˆ‘ä»¬åˆšæ‰è¯´äº†ä»€ä¹ˆï¼Ÿ")
```

### 4. Toolsï¼ˆå·¥å…·ï¼‰

æ‰©å±• AI çš„èƒ½åŠ›èŒƒå›´ï¼š

```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent

# åŠ è½½å·¥å…·
tools = load_tools([
    "serpapi",     # æœç´¢å¼•æ“
    "calculator",  # è®¡ç®—å™¨
    "wikipedia"    # ç»´åŸºç™¾ç§‘
])

# åˆ›å»ºä»£ç†
agent = initialize_agent(
    tools, 
    llm, 
    agent="zero-shot-react-description",
    verbose=True
)

# ä½¿ç”¨ä»£ç†
agent.run("2023å¹´ä¸–ç•Œæ¯å† å†›æ˜¯è°ï¼Ÿ")
```

## å®é™…åº”ç”¨æ¡ˆä¾‹ ğŸ’¡

### 1. æ™ºèƒ½å®¢æœæœºå™¨äºº

```python
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory

class CustomerServiceBot:
    def __init__(self):
        # åˆå§‹åŒ–è®°å¿†
        self.memory = ConversationBufferWindowMemory(
            k=5  # è®°ä½æœ€è¿‘5è½®å¯¹è¯
        )
        
        # åˆ›å»ºå¯¹è¯é“¾
        self.chain = ConversationChain(
            llm=OpenAI(temperature=0.7),
            memory=self.memory,
            verbose=True
        )
        
    async def handle_message(self, message: str) -> str:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        try:
            # ç”Ÿæˆå›å¤
            response = await self.chain.apredict(
                input=message
            )
            
            return response
            
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
```

### 2. æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

class DocumentAssistant:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.db = None
        
    def load_document(self, file_path: str):
        """åŠ è½½æ–‡æ¡£"""
        # 1. åŠ è½½æ–‡ä»¶
        loader = TextLoader(file_path)
        documents = loader.load()
        
        # 2. åˆ†å‰²æ–‡æœ¬
        text_splitter = CharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        # 3. åˆ›å»ºå‘é‡æ•°æ®åº“
        self.db = Chroma.from_documents(
            texts,
            self.embeddings
        )
        
    async def answer_question(self, question: str) -> str:
        """å›ç­”é—®é¢˜"""
        if not self.db:
            return "è¯·å…ˆåŠ è½½æ–‡æ¡£"
            
        # 1. æœç´¢ç›¸å…³å†…å®¹
        docs = self.db.similarity_search(question)
        
        # 2. ç”Ÿæˆå›ç­”
        chain = load_qa_chain(OpenAI(), chain_type="stuff")
        response = await chain.arun(
            input_documents=docs,
            question=question
        )
        
        return response
```

### 3. æ•°æ®åˆ†æåŠ©æ‰‹

```python
from langchain.agents import create_pandas_dataframe_agent
import pandas as pd

class DataAnalysisAssistant:
    def __init__(self):
        self.agent = None
        
    def load_data(self, file_path: str):
        """åŠ è½½æ•°æ®"""
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path)
        
        # åˆ›å»ºæ•°æ®åˆ†æä»£ç†
        self.agent = create_pandas_dataframe_agent(
            OpenAI(temperature=0),
            df,
            verbose=True
        )
        
    def analyze(self, question: str) -> str:
        """åˆ†ææ•°æ®"""
        if not self.agent:
            return "è¯·å…ˆåŠ è½½æ•°æ®"
            
        try:
            return self.agent.run(question)
        except Exception as e:
            return f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
```

## æœ€ä½³å®è·µ âœ¨

### 1. æç¤ºå·¥ç¨‹

- **æ˜ç¡®æŒ‡ä»¤**ï¼šç»™å‡ºæ¸…æ™°çš„ä»»åŠ¡æè¿°
```python
good_prompt = """
è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š
- å¦‚æœæ˜¯æ­£é¢æƒ…æ„Ÿï¼Œè¿”å›"positive"
- å¦‚æœæ˜¯è´Ÿé¢æƒ…æ„Ÿï¼Œè¿”å›"negative"
- å¦‚æœæ˜¯ä¸­æ€§æƒ…æ„Ÿï¼Œè¿”å›"neutral"

æ–‡æœ¬ï¼š{text}
"""

bad_prompt = """
åˆ†ææƒ…æ„Ÿï¼š{text}
"""
```

- **ç»“æ„åŒ–è¾“å‡º**ï¼šæŒ‡å®šè¿”å›æ ¼å¼
```python
structured_prompt = """
åˆ†æä»¥ä¸‹äº§å“è¯„ä»·ï¼Œè¿”å›JSONæ ¼å¼ï¼š
{
    "sentiment": "positive/negative/neutral",
    "key_points": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
    "rating": 1-5
}

è¯„ä»·ï¼š{review}
"""
```

### 2. é”™è¯¯å¤„ç†

```python
class RobustChain:
    def __init__(self, llm, max_retries=3):
        self.llm = llm
        self.max_retries = max_retries
        
    async def run_with_retry(self, prompt):
        """å¸¦é‡è¯•çš„è¿è¡Œ"""
        for i in range(self.max_retries):
            try:
                return await self.llm.agenerate([prompt])
            except Exception as e:
                if i == self.max_retries - 1:
                    raise e
                await asyncio.sleep(1 * (i + 1))
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
class OptimizedChain:
    def __init__(self):
        self.cache = {}
        self.lock = asyncio.Lock()
        
    async def run_with_cache(self, key, func):
        """å¸¦ç¼“å­˜çš„è¿è¡Œ"""
        # æ£€æŸ¥ç¼“å­˜
        if key in self.cache:
            return self.cache[key]
            
        # è·å–é”
        async with self.lock:
            # åŒé‡æ£€æŸ¥
            if key in self.cache:
                return self.cache[key]
                
            # æ‰§è¡Œå‡½æ•°
            result = await func()
            
            # æ›´æ–°ç¼“å­˜
            self.cache[key] = result
            return result
```

## å®ç”¨æŠ€å·§ ğŸ’ª

### 1. æ‰¹é‡å¤„ç†

```python
async def batch_process(items, chain, batch_size=5):
    """æ‰¹é‡å¤„ç†æ•°æ®"""
    results = []
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        # å¹¶è¡Œå¤„ç†
        tasks = [
            chain.arun(item)
            for item in batch
        ]
        batch_results = await asyncio.gather(*tasks)
        results.extend(batch_results)
    return results
```

### 2. æµå¼å¤„ç†

```python
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# åˆ›å»ºæµå¼LLM
streaming_llm = OpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0
)

# æµå¼é“¾
streaming_chain = LLMChain(
    llm=streaming_llm,
    prompt=prompt
)
```

### 3. è°ƒè¯•æŠ€å·§

```python
from langchain.callbacks import get_openai_callback

# è·Ÿè¸ªtokenä½¿ç”¨
with get_openai_callback() as cb:
    response = chain.run("æµ‹è¯•è¾“å…¥")
    print(f"æ€»Tokenæ•°: {cb.total_tokens}")
    print(f"æç¤ºTokenæ•°: {cb.prompt_tokens}")
    print(f"å®ŒæˆTokenæ•°: {cb.completion_tokens}")
    print(f"æ€»æˆæœ¬: ${cb.total_cost}")
```

## å°ç»“ ğŸ“

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œçµæ´»çš„æ¡†æ¶ï¼Œå®ƒèƒ½å¸®åŠ©æˆ‘ä»¬ï¼š
1. å¿«é€Ÿæ„å»º AI åº”ç”¨
2. ç»„åˆå¤šç§ AI èƒ½åŠ›
3. ä¼˜åŒ–åº”ç”¨æ€§èƒ½

å…³é”®è¦ç‚¹ï¼š
- æŒæ¡æ ¸å¿ƒç»„ä»¶
- ç†è§£æœ€ä½³å®è·µ
- æ³¨é‡å®é™…åº”ç”¨

ä¸‹ä¸€æ­¥ï¼š
- æ¢ç´¢æ›´å¤šç»„ä»¶
- å°è¯•å®é™…é¡¹ç›®
- ä¼˜åŒ–åº”ç”¨æ€§èƒ½

è®°ä½ï¼šå¥½çš„åº”ç”¨ä¸æ˜¯ä¸€è¹´è€Œå°±çš„ï¼Œéœ€è¦åœ¨å®è·µä¸­ä¸æ–­ä¼˜åŒ–å’Œæ”¹è¿›ã€‚ä»ç®€å•çš„åº”ç”¨å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§ï¼Œæœ€ç»ˆæ„å»ºå‡ºå¼ºå¤§è€Œå¯é çš„ç³»ç»Ÿã€‚
