---
title: "å‘é‡æ•°æ®åº“ç´¢å¼•æŠ€æœ¯è¯¦è§£"
slug: "indexing"
sequence: 3
description: "æ·±å…¥ç†è§£å‘é‡æ•°æ®åº“çš„ç´¢å¼•ç±»å‹ã€æ„å»ºç­–ç•¥ã€æ›´æ–°æœºåˆ¶å’Œç»´æŠ¤ä¼˜åŒ–ï¼ŒæŒæ¡æ€§èƒ½è°ƒä¼˜çš„æ ¸å¿ƒæŠ€æœ¯"
is_published: true
estimated_minutes: 45
language: "zh-CN"
permalink: "https://www.agi01.com/course/agi/vector-db/indexing"
course: "agi/course/vector-db"
header_image: "https://z1.zve.cn/tutorial/vector-db/indexing_header.png"
---

# å‘é‡æ•°æ®åº“ç´¢å¼•æŠ€æœ¯è¯¦è§£

## ä¸ºä»€ä¹ˆéœ€è¦ç´¢å¼•ï¼ŸğŸ¤”

åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œå¦‚æœæ²¡æœ‰ç´¢å¼•ï¼ŒæŸ¥è¯¢æ—¶éœ€è¦éå†æ‰€æœ‰å‘é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæš´åŠ›æœç´¢ï¼‰ï¼Œè¿™åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šæ˜¯ä¸å¯æ¥å—çš„ã€‚ç´¢å¼•çš„ç›®çš„æ˜¯ï¼š

1. **åŠ é€Ÿæ£€ç´¢**
   - å‡å°‘æ¯”è¾ƒæ¬¡æ•°
   - æé«˜æŸ¥è¯¢æ•ˆç‡
   - é™ä½èµ„æºæ¶ˆè€—

2. **æ”¯æŒè§„æ¨¡åŒ–**
   - å¤„ç†æµ·é‡æ•°æ®
   - ç»´æŒç¨³å®šæ€§èƒ½
   - ä¼˜åŒ–å†…å­˜ä½¿ç”¨

## ä¸»æµç´¢å¼•ç±»å‹ ğŸ“š

### 1. æ ‘å½¢ç´¢å¼•

#### KD-Tree
```python
from sklearn.neighbors import KDTree
import numpy as np

# åˆ›å»ºç¤ºä¾‹æ•°æ®
X = np.random.random((1000, 128))  # 1000ä¸ª128ç»´å‘é‡

# æ„å»ºKDæ ‘
tree = KDTree(X, leaf_size=40)

# æŸ¥è¯¢æœ€è¿‘é‚»
query_point = np.random.random(128)
dist, ind = tree.query([query_point], k=5)  # æŸ¥æ‰¾æœ€è¿‘çš„5ä¸ªç‚¹
```

**ä¼˜ç‚¹ï¼š**
- æ„å»ºç®€å•
- å†…å­˜å‹å¥½
- é€‚åˆä½ç»´

**ç¼ºç‚¹ï¼š**
- ç»´åº¦ç¾éš¾
- ä¸é€‚åˆé«˜ç»´
- æ›´æ–°æˆæœ¬é«˜

### 2. å“ˆå¸Œç´¢å¼•

#### LSH (å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ)
```python
import numpy as np
from datasketch import MinHashLSH

# åˆ›å»º LSH ç´¢å¼•
lsh = MinHashLSH(threshold=0.8, num_perm=128)

# æ·»åŠ æ•°æ®
for i, vec in enumerate(vectors):
    lsh.insert(f"id_{i}", vec)

# æŸ¥è¯¢ç›¸ä¼¼é¡¹
results = lsh.query(query_vector)
```

**ä¼˜ç‚¹ï¼š**
- é€Ÿåº¦å¿«
- å†…å­˜æ•ˆç‡é«˜
- æ”¯æŒæµå¼æ›´æ–°

**ç¼ºç‚¹ï¼š**
- ç²¾åº¦æŸå¤±
- éœ€è¦å¤šè¡¨
- å‚æ•°æ•æ„Ÿ

### 3. é‡åŒ–ç´¢å¼•

#### PQ (ä¹˜ç§¯é‡åŒ–)
```python
import faiss
import numpy as np

# å‡†å¤‡æ•°æ®
d = 128  # å‘é‡ç»´åº¦
nb = 10000  # æ•°æ®é‡
np.random.seed(1234)
xb = np.random.random((nb, d)).astype('float32')

# åˆ›å»º PQ ç´¢å¼•
nlist = 100  # èšç±»ä¸­å¿ƒæ•°é‡
m = 8  # å­ç©ºé—´æ•°é‡
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)

# è®­ç»ƒç´¢å¼•
index.train(xb)
index.add(xb)

# æœç´¢
k = 4  # è¿”å›æœ€è¿‘çš„4ä¸ªç»“æœ
D, I = index.search(xb[:5], k)
```

**ä¼˜ç‚¹ï¼š**
- å‹ç¼©æ•ˆæœå¥½
- æ£€ç´¢å¿«é€Ÿ
- å†…å­˜å ç”¨å°

**ç¼ºç‚¹ï¼š**
- ç²¾åº¦æŸå¤±
- è®­ç»ƒå¤æ‚
- æ›´æ–°å›°éš¾

### 4. å›¾ç´¢å¼•

#### HNSW (åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œå›¾)
```python
from hnswlib import Index

# åˆå§‹åŒ–ç´¢å¼•
dim = 128  # å‘é‡ç»´åº¦
num_elements = 10000  # æ•°æ®é‡

# åˆ›å»ºç´¢å¼•
index = Index(space='l2', dim=dim)
index.init_index(
    max_elements=num_elements,
    ef_construction=200,
    M=16
)

# æ·»åŠ æ•°æ®
index.add_items(data)

# æœç´¢
labels, distances = index.knn_query(query_data, k=5)
```

**ä¼˜ç‚¹ï¼š**
- æ€§èƒ½æœ€ä¼˜
- å†…å­˜å‹å¥½
- æ”¯æŒå¢é‡

**ç¼ºç‚¹ï¼š**
- å†…å­˜æ¶ˆè€—å¤§
- å‚æ•°å¤æ‚
- åˆ é™¤å›°éš¾

## ç´¢å¼•æ„å»ºç­–ç•¥ ğŸ”¨

### 1. æ•°æ®é¢„å¤„ç†
```python
def preprocess_vectors(vectors, normalize=True, remove_outliers=True):
    """å‘é‡é¢„å¤„ç†"""
    if normalize:
        # L2 å½’ä¸€åŒ–
        norms = np.linalg.norm(vectors, axis=1)
        vectors = vectors / norms[:, np.newaxis]
    
    if remove_outliers:
        # IQRæ–¹æ³•å»é™¤å¼‚å¸¸å€¼
        q1 = np.percentile(vectors, 25, axis=0)
        q3 = np.percentile(vectors, 75, axis=0)
        iqr = q3 - q1
        mask = ~((vectors < (q1 - 1.5 * iqr)) | 
                 (vectors > (q3 + 1.5 * iqr))).any(axis=1)
        vectors = vectors[mask]
    
    return vectors
```

### 2. åˆ†æ‰¹æ„å»º
```python
def build_index_batches(vectors, batch_size=1000):
    """åˆ†æ‰¹æ„å»ºç´¢å¼•"""
    index = faiss.IndexFlatL2(vectors.shape[1])
    
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i + batch_size]
        index.add(batch)
        print(f"å·²æ·»åŠ  {i + len(batch)} ä¸ªå‘é‡")
    
    return index
```

### 3. å¹¶è¡Œè®­ç»ƒ
```python
def train_index_parallel(vectors, n_threads=4):
    """å¹¶è¡Œè®­ç»ƒç´¢å¼•"""
    # è®¾ç½®çº¿ç¨‹æ•°
    faiss.omp_set_num_threads(n_threads)
    
    # åˆ›å»ºå’Œè®­ç»ƒç´¢å¼•
    nlist = 100
    quantizer = faiss.IndexFlatL2(vectors.shape[1])
    index = faiss.IndexIVFFlat(quantizer, vectors.shape[1], nlist)
    
    index.train(vectors)
    index.add(vectors)
    
    return index
```

## ç´¢å¼•æ›´æ–°æœºåˆ¶ ğŸ”„

### 1. å¢é‡æ›´æ–°
```python
class IncrementalIndex:
    def __init__(self, dim):
        self.main_index = faiss.IndexFlatL2(dim)
        self.buffer_index = faiss.IndexFlatL2(dim)
        self.buffer_size = 1000
        
    def add(self, vectors):
        """æ·»åŠ æ–°å‘é‡"""
        if self.buffer_index.ntotal + len(vectors) >= self.buffer_size:
            # åˆå¹¶åˆ°ä¸»ç´¢å¼•
            self._merge_indexes()
        self.buffer_index.add(vectors)
    
    def _merge_indexes(self):
        """åˆå¹¶ç´¢å¼•"""
        if self.buffer_index.ntotal > 0:
            faiss.normalize_L2(self.buffer_index)
            self.main_index.add(
                faiss.vector_to_array(self.buffer_index)
            )
            self.buffer_index.reset()
    
    def search(self, query, k):
        """æœç´¢"""
        # åœ¨ä¸¤ä¸ªç´¢å¼•ä¸­åˆ†åˆ«æœç´¢
        D1, I1 = self.main_index.search(query, k)
        D2, I2 = self.buffer_index.search(query, k)
        
        # åˆå¹¶ç»“æœ
        D = np.concatenate([D1, D2], axis=1)
        I = np.concatenate([I1, I2], axis=1)
        
        # å–æœ€å¥½çš„ k ä¸ªç»“æœ
        idx = np.argsort(D, axis=1)[:, :k]
        D = np.take_along_axis(D, idx, axis=1)
        I = np.take_along_axis(I, idx, axis=1)
        
        return D, I
```

### 2. åˆ é™¤å¤„ç†
```python
class DeletableIndex:
    def __init__(self, dim):
        self.index = faiss.IndexFlatL2(dim)
        self.id_map = {}  # æ˜ å°„åŸå§‹IDåˆ°ç´¢å¼•ä½ç½®
        self.deleted = set()  # å·²åˆ é™¤çš„IDé›†åˆ
    
    def add(self, vectors, ids):
        """æ·»åŠ å‘é‡"""
        for i, id in enumerate(ids):
            if id not in self.deleted:
                pos = self.index.ntotal
                self.index.add(vectors[i:i+1])
                self.id_map[id] = pos
    
    def delete(self, ids):
        """æ ‡è®°åˆ é™¤"""
        self.deleted.update(ids)
    
    def search(self, query, k):
        """æœç´¢ï¼ˆæ’é™¤å·²åˆ é™¤çš„ç»“æœï¼‰"""
        D, I = self.index.search(query, k + len(self.deleted))
        
        # è¿‡æ»¤å·²åˆ é™¤çš„ç»“æœ
        valid_results = []
        for distances, indices in zip(D, I):
            valid = [(d, i) for d, i in zip(distances, indices)
                    if i not in self.deleted][:k]
            valid_results.append(valid)
        
        return np.array(valid_results)
    
    def rebuild(self):
        """é‡å»ºç´¢å¼•ï¼ˆæ¸…ç†å·²åˆ é™¤çš„å‘é‡ï¼‰"""
        valid_vectors = []
        valid_ids = []
        
        for id, pos in self.id_map.items():
            if id not in self.deleted:
                vector = faiss.vector_to_array(
                    self.index.reconstruct(pos)
                )
                valid_vectors.append(vector)
                valid_ids.append(id)
        
        # é‡æ–°åˆ›å»ºç´¢å¼•
        self.index = faiss.IndexFlatL2(self.index.d)
        self.id_map = {}
        self.deleted = set()
        
        # æ·»åŠ æœ‰æ•ˆå‘é‡
        self.add(np.array(valid_vectors), valid_ids)
```

## ç»´æŠ¤ä¼˜åŒ– ğŸ› ï¸

### 1. æ€§èƒ½ç›‘æ§
```python
class IndexMonitor:
    def __init__(self, index):
        self.index = index
        self.metrics = {
            'query_times': [],
            'memory_usage': [],
            'accuracy': []
        }
    
    def benchmark_query(self, queries, ground_truth, k=10):
        """æ€§èƒ½æµ‹è¯•"""
        start_time = time.time()
        results = self.index.search(queries, k)
        query_time = time.time() - start_time
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = self._calculate_accuracy(results, ground_truth)
        
        # è®°å½•æŒ‡æ ‡
        self.metrics['query_times'].append(query_time)
        self.metrics['accuracy'].append(accuracy)
        
        return {
            'query_time': query_time,
            'accuracy': accuracy,
            'qps': len(queries) / query_time
        }
    
    def _calculate_accuracy(self, results, ground_truth):
        """è®¡ç®—å‡†ç¡®ç‡"""
        correct = 0
        total = len(results) * len(results[0])
        
        for pred, true in zip(results, ground_truth):
            correct += len(set(pred) & set(true))
        
        return correct / total
```

### 2. è‡ªåŠ¨è°ƒä¼˜
```python
def auto_tune_index(index, train_data, val_data, param_grid):
    """è‡ªåŠ¨è°ƒä¼˜ç´¢å¼•å‚æ•°"""
    best_score = 0
    best_params = None
    
    for params in param_grid:
        # ä½¿ç”¨å½“å‰å‚æ•°é…ç½®
        index.set_params(**params)
        
        # è®­ç»ƒç´¢å¼•
        index.train(train_data)
        
        # è¯„ä¼°æ€§èƒ½
        score = evaluate_index(index, val_data)
        
        if score > best_score:
            best_score = score
            best_params = params
    
    # ä½¿ç”¨æœ€ä½³å‚æ•°
    index.set_params(**best_params)
    return best_params, best_score
```

### 3. å®šæœŸç»´æŠ¤
```python
class IndexMaintenance:
    def __init__(self, index):
        self.index = index
        self.last_maintenance = time.time()
        self.maintenance_interval = 86400  # 24å°æ—¶
    
    def need_maintenance(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç»´æŠ¤"""
        return (time.time() - self.last_maintenance) > self.maintenance_interval
    
    def perform_maintenance(self):
        """æ‰§è¡Œç»´æŠ¤ä»»åŠ¡"""
        if self.need_maintenance():
            # 1. å¤‡ä»½ç´¢å¼•
            self._backup_index()
            
            # 2. æ•´ç†ç¢ç‰‡
            self._defragment()
            
            # 3. é‡å»ºç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self._fragmentation_ratio() > 0.3:
                self._rebuild_index()
            
            # 4. æ›´æ–°ç»´æŠ¤æ—¶é—´
            self.last_maintenance = time.time()
    
    def _backup_index(self):
        """å¤‡ä»½ç´¢å¼•"""
        backup_path = f"index_backup_{int(time.time())}"
        faiss.write_index(self.index, backup_path)
    
    def _defragment(self):
        """æ•´ç†ç¢ç‰‡"""
        # å®ç°ç¢ç‰‡æ•´ç†é€»è¾‘
        pass
    
    def _fragmentation_ratio(self):
        """è®¡ç®—ç¢ç‰‡ç‡"""
        # å®ç°ç¢ç‰‡ç‡è®¡ç®—é€»è¾‘
        return 0.0
    
    def _rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        # å®ç°ç´¢å¼•é‡å»ºé€»è¾‘
        pass
```

## æœ€ä½³å®è·µ ğŸ’¡

### 1. é€‰æ‹©ç´¢å¼•ç±»å‹
- **æ•°æ®é‡å°äº100ä¸‡**ï¼šä½¿ç”¨ HNSW
- **æ•°æ®é‡100ä¸‡-1000ä¸‡**ï¼šä½¿ç”¨ IVF + PQ
- **æ•°æ®é‡å¤§äº1000ä¸‡**ï¼šä½¿ç”¨åˆ†å¸ƒå¼ç´¢å¼•

### 2. è°ƒä¼˜å»ºè®®
- **å†…å­˜å—é™**ï¼šä½¿ç”¨ PQ å‹ç¼©
- **é€Ÿåº¦ä¼˜å…ˆ**ï¼šä½¿ç”¨ HNSW
- **ç²¾åº¦ä¼˜å…ˆ**ï¼šä½¿ç”¨ IVF + Flat

### 3. æ³¨æ„äº‹é¡¹
- å®šæœŸç›‘æ§æ€§èƒ½
- åˆç†è®¾ç½®å‚æ•°
- åšå¥½å®¹é‡è§„åˆ’
- å»ºç«‹å¤‡ä»½æœºåˆ¶

## å°ç»“ ğŸ“

æœ¬ç« æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†å‘é‡æ•°æ®åº“çš„ç´¢å¼•æŠ€æœ¯ï¼š

1. **ç´¢å¼•ç±»å‹**
   - æ ‘å½¢ç´¢å¼•
   - å“ˆå¸Œç´¢å¼•
   - é‡åŒ–ç´¢å¼•
   - å›¾ç´¢å¼•

2. **æ„å»ºç­–ç•¥**
   - æ•°æ®é¢„å¤„ç†
   - åˆ†æ‰¹æ„å»º
   - å¹¶è¡Œè®­ç»ƒ

3. **æ›´æ–°æœºåˆ¶**
   - å¢é‡æ›´æ–°
   - åˆ é™¤å¤„ç†
   - é‡å»ºä¼˜åŒ–

4. **ç»´æŠ¤ä¼˜åŒ–**
   - æ€§èƒ½ç›‘æ§
   - è‡ªåŠ¨è°ƒä¼˜
   - å®šæœŸç»´æŠ¤

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä¼˜åŒ–å‘é‡æ•°æ®åº“çš„æŸ¥è¯¢æ€§èƒ½ï¼
